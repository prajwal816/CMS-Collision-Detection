{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14858164,"sourceType":"datasetVersion","datasetId":9504346}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Phase 4 — Event-level evaluation + Multi-label training + Robustness package\n\nGoal of Phase 4:\n1) Convert your **pair-level** table into **event-level** predictions (no “multiple pairs per event” confusion)\n2) Train and evaluate **multiple HLT labels** (1 model per label) with the SAME GroupKFold splits\n3) Add robustness: calibration, threshold selection, drift vs run/lumi, and clean artifacts export\n\nInput: `/kaggle/working/parquet_dimuon/*.parquet` (from Phase 2)\n\nOutput (saved to `/kaggle/working/phase4_artifacts/`):\n- `metrics_per_label.csv` (pair-level + event-level)\n- `thresholds.csv` (operating points)\n- `models/` (joblib models per label)\n- `plots/` (ROC/PR, stability, calibration)\n- `config.json`\n","metadata":{}},{"cell_type":"code","source":"# Cell 1 — Install deps\n!pip -q install lightgbm scikit-learn shap pyarrow fastparquet matplotlib seaborn joblib\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T05:52:34.014202Z","iopub.execute_input":"2026-02-17T05:52:34.014454Z","iopub.status.idle":"2026-02-17T05:52:38.823509Z","shell.execute_reply.started":"2026-02-17T05:52:34.014424Z","shell.execute_reply":"2026-02-17T05:52:38.822642Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2 — Imports & config\nfrom pathlib import Path\nimport glob, json\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import (\n    roc_auc_score, average_precision_score,\n    roc_curve, precision_recall_curve,\n    brier_score_loss\n)\nfrom sklearn.calibration import calibration_curve\nimport lightgbm as lgb\nimport joblib\n\nSEED = 42\nnp.random.seed(SEED)\n\nPARQUET_DIR = Path(\"/kaggle/input/datasets/katakuricharlotte/parquet-triggeremu/parquet_dimuon\")\nOUT = Path(\"/kaggle/working/phase4_artifacts\")\n(OUT / \"models\").mkdir(parents=True, exist_ok=True)\n(OUT / \"plots\").mkdir(parents=True, exist_ok=True)\n\nparquet_files = sorted(glob.glob(str(PARQUET_DIR / \"*.parquet\")))\nlen(parquet_files), parquet_files[:3]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3 — Load Parquet (scale-friendly)\n# Start with all shards; if too big, cap MAX_FILES.\nMAX_FILES = None  # set e.g. 5 if you want faster iteration\nuse_files = parquet_files if MAX_FILES is None else parquet_files[:MAX_FILES]\n\ndf = pd.concat([pd.read_parquet(p) for p in use_files], ignore_index=True)\ndf.shape, df.head(3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4 — Basic data validation\nrequired_cols = {\"run\",\"lumi\",\"event\"}\nassert required_cols.issubset(df.columns), f\"Missing keys: {required_cols - set(df.columns)}\"\n\nLABELS = sorted([c for c in df.columns if c.startswith(\"HLT_\")])\nassert len(LABELS) > 0, \"No HLT labels found.\"\n\n# event_id for grouping\ndf[\"event_id\"] = df[\"run\"].astype(str) + \":\" + df[\"lumi\"].astype(str) + \":\" + df[\"event\"].astype(str)\n\nprint(\"rows:\", len(df))\nprint(\"unique events:\", df[\"event_id\"].nunique())\nprint(\"labels:\", LABELS[:5], \"... total:\", len(LABELS))\n\n(df[LABELS].mean().sort_values(ascending=False).head(10))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4-A: Define features (explainable & stable)\n\nWe keep features that are:\n- physically meaningful (m_mumu, pt_mumu, dR, etc.)\n- simple context (PV_npvs, MET_pt)\nNo IDs and no HLT columns.\n","metadata":{}},{"cell_type":"code","source":"# Cell 5 — Feature set and simple preprocessing\ndrop_cols = set([\"run\",\"lumi\",\"event\",\"event_id\"]) | set(LABELS)\nfeatures = [c for c in df.columns if c not in drop_cols]\n\nX = df[features].copy()\nfor c in X.columns:\n    if X[c].dtype == \"object\":\n        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n\n# Simple median fill for baseline robustness\nX = X.fillna(X.median(numeric_only=True))\n\nX.shape, features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6 — Utility: metrics at pair-level and event-level\ndef safe_auc(y_true, y_score):\n    # Handle edge case where a fold has only one class\n    if len(np.unique(y_true)) < 2:\n        return np.nan\n    return roc_auc_score(y_true, y_score)\n\ndef compute_metrics(y_true, y_score):\n    return {\n        \"roc_auc\": safe_auc(y_true, y_score),\n        \"ap\": average_precision_score(y_true, y_score),\n        \"brier\": brier_score_loss(y_true, y_score),\n        \"pos_rate\": float(np.mean(y_true)),\n        \"n\": int(len(y_true)),\n    }\n\ndef event_level_aggregate(df_part, score_col=\"score\", label_col=\"y\"):\n    # Aggregate pair rows to event-level:\n    # - event score = max(score) (if any pair \"looks like\" it should fire, event fires)\n    # - event label = max(label) (event-level label repeated, so max is safe)\n    g = df_part.groupby(\"event_id\", as_index=False).agg(\n        y=(label_col, \"max\"),\n        score=(score_col, \"max\"),\n        run=(\"run\", \"first\"),\n        lumi=(\"lumi\", \"first\"),\n        event=(\"event\", \"first\"),\n    )\n    return g\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4-B: Consistent GroupKFold splits (by event)\n\nWe split by `event_id` so that the same collision event never appears in both train and validation.\n","metadata":{}},{"cell_type":"code","source":"# Cell 7 — Prepare GroupKFold splits on unique events (stronger than splitting pair rows)\nevents_unique = df[[\"event_id\"]].drop_duplicates().reset_index(drop=True)\nevent_ids = events_unique[\"event_id\"].values\n\ngkf = GroupKFold(n_splits=5)\n\nsplits = list(gkf.split(event_ids, np.zeros(len(event_ids)), groups=event_ids))\nlen(splits), [ (len(tr), len(va)) for tr,va in splits ]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8 — Map event splits to pair-row indices (so training uses pair rows but validation stays event-disjoint)\nevent_to_fold = {}\nfor fold, (_, va_idx) in enumerate(splits, start=1):\n    for eid in event_ids[va_idx]:\n        event_to_fold[eid] = fold\n\ndf[\"fold\"] = df[\"event_id\"].map(event_to_fold).astype(int)\ndf[\"fold\"].value_counts().sort_index()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4-C: Train 1 model per label (with class imbalance handling)\n\nWe train LightGBM models with `scale_pos_weight` for imbalance, and we store:\n- per-fold metrics\n- overall OOF predictions\n- event-level metrics after aggregating pair scores to event scores\n","metadata":{}},{"cell_type":"code","source":"# Cell 9 — Training loop for multiple labels\nparams = dict(\n    n_estimators=800,\n    learning_rate=0.05,\n    num_leaves=31,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=SEED,\n    n_jobs=-1,\n)\n\nall_metrics = []\noof_scores = {}   # label -> oof scores (pair-level)\nmodels = {}\n\nfor label in LABELS:\n    y = df[label].astype(int).values\n\n    # Skip degenerate labels\n    if y.mean() < 1e-4 or y.mean() > 1 - 1e-4:\n        all_metrics.append({\"label\": label, \"note\": \"skipped_degenerate\", \"pos_rate\": float(y.mean())})\n        continue\n\n    oof = np.zeros(len(df), dtype=float)\n\n    for fold in sorted(df[\"fold\"].unique()):\n        tr_mask = df[\"fold\"].values != fold\n        va_mask = df[\"fold\"].values == fold\n\n        X_tr, X_va = X.loc[tr_mask], X.loc[va_mask]\n        y_tr, y_va = y[tr_mask], y[va_mask]\n\n        pos = max(y_tr.sum(), 1)\n        neg = max(len(y_tr) - y_tr.sum(), 1)\n        spw = neg / pos\n\n        clf = lgb.LGBMClassifier(objective=\"binary\", scale_pos_weight=spw, **params)\n        clf.fit(X_tr, y_tr)\n\n        pred = clf.predict_proba(X_va)[:, 1]\n        oof[va_mask] = pred\n\n        m_pair = compute_metrics(y_va, pred)\n        all_metrics.append({\n            \"label\": label,\n            \"fold\": int(fold),\n            \"level\": \"pair\",\n            \"scale_pos_weight\": float(spw),\n            **m_pair\n        })\n\n    # Store OOF\n    oof_scores[label] = oof\n\n    # Train final model on all data\n    pos = max(y.sum(), 1)\n    neg = max(len(y) - y.sum(), 1)\n    spw = neg / pos\n    final_model = lgb.LGBMClassifier(objective=\"binary\", scale_pos_weight=spw, **params)\n    final_model.fit(X, y)\n\n    models[label] = final_model\n    joblib.dump(final_model, OUT / \"models\" / f\"{label}.joblib\")\n\nlen(models), list(models.keys())[:5]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10 — Compute event-level metrics from OOF (per label)\nevent_metrics = []\n\nfor label, oof in oof_scores.items():\n    tmp = df[[\"event_id\",\"run\",\"lumi\",\"event\"]].copy()\n    tmp[\"y\"] = df[label].astype(int).values\n    tmp[\"score\"] = oof\n\n    ev = event_level_aggregate(tmp, score_col=\"score\", label_col=\"y\")\n    m_ev = compute_metrics(ev[\"y\"].values, ev[\"score\"].values)\n\n    event_metrics.append({\"label\": label, \"level\": \"event\", \"fold\": \"OOF\", **m_ev})\n\nevent_metrics_df = pd.DataFrame(event_metrics).sort_values([\"ap\"], ascending=False)\nevent_metrics_df.head(15)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11 — Combine and save metrics\nmetrics_df = pd.DataFrame(all_metrics)\nmetrics_out = OUT / \"metrics_per_label.csv\"\nmetrics_df.to_csv(metrics_out, index=False)\n\nevent_out = OUT / \"event_metrics_oof.csv\"\nevent_metrics_df.to_csv(event_out, index=False)\n\nmetrics_out, event_out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4-D: Choose operating thresholds (engineering deliverable)\n\nWe pick a threshold for each label based on a target **recall** (efficiency) or based on maximizing F1.\nThis makes the model usable like a trigger/selection component.\n","metadata":{}},{"cell_type":"code","source":"# Cell 12 — Threshold selection (OOF) per label\ndef choose_threshold(y_true, y_score, target_recall=0.95):\n    prec, rec, thr = precision_recall_curve(y_true, y_score)\n    # precision_recall_curve returns thr of length n-1\n    # Find first threshold reaching recall >= target_recall\n    idx = np.where(rec[:-1] >= target_recall)[0]\n    if len(idx) == 0:\n        # fallback: best F1\n        f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n        best = np.nanargmax(f1)\n        return float(thr[max(best-1, 0)]), float(prec[best]), float(rec[best])\n    i = idx[0]\n    return float(thr[i]), float(prec[i]), float(rec[i])\n\nthreshold_rows = []\nfor label, oof in oof_scores.items():\n    tmp = df[[\"event_id\"]].copy()\n    tmp[\"y\"] = df[label].astype(int).values\n    tmp[\"score\"] = oof\n    ev = event_level_aggregate(tmp, score_col=\"score\", label_col=\"y\")\n\n    th, p, r = choose_threshold(ev[\"y\"].values, ev[\"score\"].values, target_recall=0.95)\n    threshold_rows.append({\"label\": label, \"threshold\": th, \"precision_at_th\": p, \"recall_at_th\": r, \"event_pos_rate\": float(ev[\"y\"].mean())})\n\nthresholds_df = pd.DataFrame(threshold_rows).sort_values(\"event_pos_rate\", ascending=False)\nthresholds_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 13 — Save thresholds\nthr_out = OUT / \"thresholds.csv\"\nthresholds_df.to_csv(thr_out, index=False)\nthr_out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4-E: Plots (ROC/PR + calibration) for top labels\nWe generate plots for the top 3 labels by event-level AP (OOF).\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}