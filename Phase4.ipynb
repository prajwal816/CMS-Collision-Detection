{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14858164,"sourceType":"datasetVersion","datasetId":9504346}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Phase 4 — Event-level evaluation + Multi-label training + Robustness package\n\nGoal of Phase 4:\n1) Convert your **pair-level** table into **event-level** predictions (no “multiple pairs per event” confusion)\n2) Train and evaluate **multiple HLT labels** (1 model per label) with the SAME GroupKFold splits\n3) Add robustness: calibration, threshold selection, drift vs run/lumi, and clean artifacts export\n\nInput: `/kaggle/working/parquet_dimuon/*.parquet` (from Phase 2)\n\nOutput (saved to `/kaggle/working/phase4_artifacts/`):\n- `metrics_per_label.csv` (pair-level + event-level)\n- `thresholds.csv` (operating points)\n- `models/` (joblib models per label)\n- `plots/` (ROC/PR, stability, calibration)\n- `config.json`\n","metadata":{}},{"cell_type":"code","source":"# Cell 1 — Install deps\n!pip -q install lightgbm scikit-learn shap pyarrow fastparquet matplotlib seaborn joblib\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2 — Imports & config\nfrom pathlib import Path\nimport glob, json\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import (\n    roc_auc_score, average_precision_score,\n    roc_curve, precision_recall_curve,\n    brier_score_loss\n)\nfrom sklearn.calibration import calibration_curve\nimport lightgbm as lgb\nimport joblib\n\nSEED = 42\nnp.random.seed(SEED)\n\nPARQUET_DIR = Path(\"/kaggle/input/datasets/katakuricharlotte/parquet-triggeremu/parquet_dimuon\")\nOUT = Path(\"/kaggle/working/phase4_artifacts\")\n(OUT / \"models\").mkdir(parents=True, exist_ok=True)\n(OUT / \"plots\").mkdir(parents=True, exist_ok=True)\n\nparquet_files = sorted(glob.glob(str(PARQUET_DIR / \"*.parquet\")))\nlen(parquet_files), parquet_files[:3]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3 — Load Parquet (scale-friendly)\n# Start with all shards; if too big, cap MAX_FILES.\nMAX_FILES = None  # set e.g. 5 if you want faster iteration\nuse_files = parquet_files if MAX_FILES is None else parquet_files[:MAX_FILES]\n\ndf = pd.concat([pd.read_parquet(p) for p in use_files], ignore_index=True)\ndf.shape, df.head(3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4 — Basic data validation\nrequired_cols = {\"run\",\"lumi\",\"event\"}\nassert required_cols.issubset(df.columns), f\"Missing keys: {required_cols - set(df.columns)}\"\n\nLABELS = sorted([c for c in df.columns if c.startswith(\"HLT_\")])\nassert len(LABELS) > 0, \"No HLT labels found.\"\n\n# event_id for grouping\ndf[\"event_id\"] = df[\"run\"].astype(str) + \":\" + df[\"lumi\"].astype(str) + \":\" + df[\"event\"].astype(str)\n\nprint(\"rows:\", len(df))\nprint(\"unique events:\", df[\"event_id\"].nunique())\nprint(\"labels:\", LABELS[:5], \"... total:\", len(LABELS))\n\n(df[LABELS].mean().sort_values(ascending=False).head(10))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4-A: Define features (explainable & stable)\n\nWe keep features that are:\n- physically meaningful (m_mumu, pt_mumu, dR, etc.)\n- simple context (PV_npvs, MET_pt)\nNo IDs and no HLT columns.\n","metadata":{}},{"cell_type":"code","source":"# Cell 5 — Feature set and simple preprocessing\ndrop_cols = set([\"run\",\"lumi\",\"event\",\"event_id\"]) | set(LABELS)\nfeatures = [c for c in df.columns if c not in drop_cols]\n\nX = df[features].copy()\nfor c in X.columns:\n    if X[c].dtype == \"object\":\n        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n\n# Simple median fill for baseline robustness\nX = X.fillna(X.median(numeric_only=True))\n\nX.shape, features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6 — Utility: metrics at pair-level and event-level\ndef safe_auc(y_true, y_score):\n    # Handle edge case where a fold has only one class\n    if len(np.unique(y_true)) < 2:\n        return np.nan\n    return roc_auc_score(y_true, y_score)\n\ndef compute_metrics(y_true, y_score):\n    return {\n        \"roc_auc\": safe_auc(y_true, y_score),\n        \"ap\": average_precision_score(y_true, y_score),\n        \"brier\": brier_score_loss(y_true, y_score),\n        \"pos_rate\": float(np.mean(y_true)),\n        \"n\": int(len(y_true)),\n    }\n\ndef event_level_aggregate(df_part, score_col=\"score\", label_col=\"y\"):\n    # Aggregate pair rows to event-level:\n    # - event score = max(score) (if any pair \"looks like\" it should fire, event fires)\n    # - event label = max(label) (event-level label repeated, so max is safe)\n    g = df_part.groupby(\"event_id\", as_index=False).agg(\n        y=(label_col, \"max\"),\n        score=(score_col, \"max\"),\n        run=(\"run\", \"first\"),\n        lumi=(\"lumi\", \"first\"),\n        event=(\"event\", \"first\"),\n    )\n    return g\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4-B: Consistent GroupKFold splits (by event)\n\nWe split by `event_id` so that the same collision event never appears in both train and validation.\n","metadata":{}},{"cell_type":"code","source":"# Cell 7 — Prepare GroupKFold splits on unique events (stronger than splitting pair rows)\nevents_unique = df[[\"event_id\"]].drop_duplicates().reset_index(drop=True)\nevent_ids = events_unique[\"event_id\"].values\n\ngkf = GroupKFold(n_splits=5)\n\nsplits = list(gkf.split(event_ids, np.zeros(len(event_ids)), groups=event_ids))\nlen(splits), [ (len(tr), len(va)) for tr,va in splits ]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8 — Map event splits to pair-row indices (so training uses pair rows but validation stays event-disjoint)\nevent_to_fold = {}\nfor fold, (_, va_idx) in enumerate(splits, start=1):\n    for eid in event_ids[va_idx]:\n        event_to_fold[eid] = fold\n\ndf[\"fold\"] = df[\"event_id\"].map(event_to_fold).astype(int)\ndf[\"fold\"].value_counts().sort_index()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4-C: Train 1 model per label (with class imbalance handling)\n\nWe train LightGBM models with `scale_pos_weight` for imbalance, and we store:\n- per-fold metrics\n- overall OOF predictions\n- event-level metrics after aggregating pair scores to event scores\n","metadata":{}},{"cell_type":"code","source":"# Cell 9 — Training loop for multiple labels\nparams = dict(\n    n_estimators=800,\n    learning_rate=0.05,\n    num_leaves=31,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=SEED,\n    n_jobs=-1,\n)\n\nall_metrics = []\noof_scores = {}   # label -> oof scores (pair-level)\nmodels = {}\n\nfor label in LABELS:\n    y = df[label].astype(int).values\n\n    # Skip degenerate labels\n    if y.mean() < 1e-4 or y.mean() > 1 - 1e-4:\n        all_metrics.append({\"label\": label, \"note\": \"skipped_degenerate\", \"pos_rate\": float(y.mean())})\n        continue\n\n    oof = np.zeros(len(df), dtype=float)\n\n    for fold in sorted(df[\"fold\"].unique()):\n        tr_mask = df[\"fold\"].values != fold\n        va_mask = df[\"fold\"].values == fold\n\n        X_tr, X_va = X.loc[tr_mask], X.loc[va_mask]\n        y_tr, y_va = y[tr_mask], y[va_mask]\n\n        pos = max(y_tr.sum(), 1)\n        neg = max(len(y_tr) - y_tr.sum(), 1)\n        spw = neg / pos\n\n        clf = lgb.LGBMClassifier(objective=\"binary\", scale_pos_weight=spw, **params)\n        clf.fit(X_tr, y_tr)\n\n        pred = clf.predict_proba(X_va)[:, 1]\n        oof[va_mask] = pred\n\n        m_pair = compute_metrics(y_va, pred)\n        all_metrics.append({\n            \"label\": label,\n            \"fold\": int(fold),\n            \"level\": \"pair\",\n            \"scale_pos_weight\": float(spw),\n            **m_pair\n        })\n\n    # Store OOF\n    oof_scores[label] = oof\n\n    # Train final model on all data\n    pos = max(y.sum(), 1)\n    neg = max(len(y) - y.sum(), 1)\n    spw = neg / pos\n    final_model = lgb.LGBMClassifier(objective=\"binary\", scale_pos_weight=spw, **params)\n    final_model.fit(X, y)\n\n    models[label] = final_model\n    joblib.dump(final_model, OUT / \"models\" / f\"{label}.joblib\")\n\nlen(models), list(models.keys())[:5]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10 — Compute event-level metrics from OOF (per label)\nevent_metrics = []\n\nfor label, oof in oof_scores.items():\n    tmp = df[[\"event_id\",\"run\",\"lumi\",\"event\"]].copy()\n    tmp[\"y\"] = df[label].astype(int).values\n    tmp[\"score\"] = oof\n\n    ev = event_level_aggregate(tmp, score_col=\"score\", label_col=\"y\")\n    m_ev = compute_metrics(ev[\"y\"].values, ev[\"score\"].values)\n\n    event_metrics.append({\"label\": label, \"level\": \"event\", \"fold\": \"OOF\", **m_ev})\n\nevent_metrics_df = pd.DataFrame(event_metrics).sort_values([\"ap\"], ascending=False)\nevent_metrics_df.head(15)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11 — Combine and save metrics\nmetrics_df = pd.DataFrame(all_metrics)\nmetrics_out = OUT / \"metrics_per_label.csv\"\nmetrics_df.to_csv(metrics_out, index=False)\n\nevent_out = OUT / \"event_metrics_oof.csv\"\nevent_metrics_df.to_csv(event_out, index=False)\n\nmetrics_out, event_out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4-D: Choose operating thresholds (engineering deliverable)\n\nWe pick a threshold for each label based on a target **recall** (efficiency) or based on maximizing F1.\nThis makes the model usable like a trigger/selection component.\n","metadata":{}},{"cell_type":"code","source":"# Cell 12 — Threshold selection (OOF) per label (FIXED)\n\ndef choose_threshold(y_true, y_score, target_recall=0.95):\n    prec, rec, thr = precision_recall_curve(y_true, y_score)\n    idx = np.where(rec[:-1] >= target_recall)[0]\n    if len(idx) == 0:\n        f1 = 2 * (prec * rec) / (prec + rec + 1e-12)\n        best = np.nanargmax(f1)\n        # thr has length len(prec)-1; protect index\n        j = max(best - 1, 0)\n        return float(thr[j]), float(prec[best]), float(rec[best])\n    i = idx[0]\n    return float(thr[i]), float(prec[i]), float(rec[i])\n\nthreshold_rows = []\n\nfor label, oof in oof_scores.items():\n    # IMPORTANT: include run/lumi/event because event_level_aggregate expects them\n    tmp = df[[\"event_id\", \"run\", \"lumi\", \"event\"]].copy()\n    tmp[\"y\"] = df[label].astype(int).values\n    tmp[\"score\"] = oof\n\n    ev = event_level_aggregate(tmp, score_col=\"score\", label_col=\"y\")\n\n    th, p, r = choose_threshold(\n        ev[\"y\"].values,\n        ev[\"score\"].values,\n        target_recall=0.95\n    )\n\n    threshold_rows.append({\n        \"label\": label,\n        \"threshold\": th,\n        \"precision_at_th\": p,\n        \"recall_at_th\": r,\n        \"event_pos_rate\": float(ev[\"y\"].mean()),\n        \"n_events\": int(len(ev))\n    })\n\nthresholds_df = (\n    pd.DataFrame(threshold_rows)\n      .sort_values(\"event_pos_rate\", ascending=False)\n      .reset_index(drop=True)\n)\n\nthresholds_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 13 — Save thresholds\nthr_out = OUT / \"thresholds.csv\"\nthresholds_df.to_csv(thr_out, index=False)\nthr_out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4-E: Plots (ROC/PR + calibration) for top labels\nWe generate plots for the top 3 labels by event-level AP (OOF).\n","metadata":{}},{"cell_type":"code","source":"# Cell 14 — Plot helpers\ndef save_roc_pr(y_true, y_score, title, out_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_score)\n    prec, rec, _ = precision_recall_curve(y_true, y_score)\n\n    plt.figure(figsize=(6,5))\n    plt.plot(fpr, tpr)\n    plt.plot([0,1],[0,1],\"--\",alpha=0.5)\n    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n    plt.title(f\"ROC: {title}\")\n    plt.tight_layout()\n    plt.savefig(out_prefix + \"_roc.png\", dpi=150)\n    plt.show()\n\n    plt.figure(figsize=(6,5))\n    plt.plot(rec, prec)\n    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n    plt.title(f\"PR: {title}\")\n    plt.tight_layout()\n    plt.savefig(out_prefix + \"_pr.png\", dpi=150)\n    plt.show()\n\ndef save_calibration(y_true, y_score, title, out_prefix):\n    frac_pos, mean_pred = calibration_curve(y_true, y_score, n_bins=10, strategy=\"quantile\")\n    plt.figure(figsize=(6,5))\n    plt.plot(mean_pred, frac_pos, marker=\"o\")\n    plt.plot([0,1],[0,1],\"--\",alpha=0.5)\n    plt.xlabel(\"Mean predicted probability\")\n    plt.ylabel(\"Fraction of positives\")\n    plt.title(f\"Calibration: {title}\")\n    plt.tight_layout()\n    plt.savefig(out_prefix + \"_cal.png\", dpi=150)\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 15 — Generate plots for top labels\ntop_labels = event_metrics_df.sort_values(\"ap\", ascending=False)[\"label\"].head(3).tolist()\ntop_labels\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 16 — Plot OOF event-level ROC/PR/Calibration for top labels (FIXED)\n\nfor label in top_labels:\n    oof = oof_scores[label]\n\n    # IMPORTANT: include run/lumi/event because event_level_aggregate expects them\n    tmp = df[[\"event_id\", \"run\", \"lumi\", \"event\"]].copy()\n    tmp[\"y\"] = df[label].astype(int).values\n    tmp[\"score\"] = oof\n\n    ev = event_level_aggregate(tmp, score_col=\"score\", label_col=\"y\")\n\n    title = f\"{label} (event-level OOF)\"\n    safe_name = label.replace(\"/\", \"_\")\n    out_prefix = str(OUT / \"plots\" / safe_name)\n\n    save_roc_pr(ev[\"y\"].values, ev[\"score\"].values, title, out_prefix)\n    save_calibration(ev[\"y\"].values, ev[\"score\"].values, title, out_prefix)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4-F: Drift monitoring (event-level) vs run and lumi\nWe look at:\n- label rate vs run bin\n- mean score vs run bin\n\n(You can later add lumi-level or time-ordered drift tests.)\n","metadata":{}},{"cell_type":"code","source":"# Cell 17 — Drift table & plots for the main label (best AP)\nmain_label = top_labels[0]\noof = oof_scores[main_label]\n\ntmp = df[[\"event_id\",\"run\",\"lumi\",\"event\"]].copy()\ntmp[\"y\"] = df[main_label].astype(int).values\ntmp[\"score\"] = oof\nev = event_level_aggregate(tmp, score_col=\"score\", label_col=\"y\")\n\nev[\"run_bin\"] = pd.cut(ev[\"run\"], bins=10)\n\nstab = ev.groupby(\"run_bin\").agg(\n    n=(\"y\",\"size\"),\n    y_rate=(\"y\",\"mean\"),\n    score_mean=(\"score\",\"mean\")\n).reset_index()\n\nstab\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 18 — Plot drift vs run bins (drop empty bins)\nstab2 = stab[stab[\"n\"] > 0].copy()\n\nplt.figure(figsize=(10,4))\nplt.plot(stab2[\"run_bin\"].astype(str), stab2[\"y_rate\"], marker=\"o\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.ylabel(\"Event label rate\")\nplt.title(f\"Label rate vs run bin — {main_label}\")\nplt.tight_layout()\nplt.savefig(OUT / \"plots\" / f\"{main_label}_drift_labelrate.png\", dpi=150)\nplt.show()\n\nplt.figure(figsize=(10,4))\nplt.plot(stab2[\"run_bin\"].astype(str), stab2[\"score_mean\"], marker=\"o\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.ylabel(\"Mean predicted score (OOF)\")\nplt.title(f\"Mean score vs run bin — {main_label}\")\nplt.tight_layout()\nplt.savefig(OUT / \"plots\" / f\"{main_label}_drift_score.png\", dpi=150)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 4-G: Robust SHAP cell (no confusing warnings)\n\nSHAP output shapes can differ for LightGBM binary classification (sometimes list, sometimes array),\nso we handle both safely before plotting.\n","metadata":{}},{"cell_type":"code","source":"# Cell 19 — SHAP (safe handling)\nimport shap\n\nlabel = main_label\nmodel = models[label]\n\n# Use a small sample of rows for speed\nsample_n = min(5000, len(X))\nidx = np.random.choice(len(X), sample_n, replace=False)\nX_s = X.iloc[idx]\n\nexplainer = shap.TreeExplainer(model)\nsv = explainer.shap_values(X_s)\n\n# Some versions return list for binary class; take the positive-class component if needed. [web:159]\nif isinstance(sv, list) and len(sv) == 2:\n    sv_use = sv[1]\nelse:\n    sv_use = sv\n\nshap.summary_plot(sv_use, X_s, show=False)\nplt.tight_layout()\nplt.savefig(OUT / \"plots\" / f\"{label}_shap_summary.png\", dpi=150)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 20 — Save config + final directory listing\nconfig = {\n    \"seed\": SEED,\n    \"n_rows\": int(len(df)),\n    \"n_unique_events\": int(df[\"event_id\"].nunique()),\n    \"features\": list(X.columns),\n    \"labels_trained\": list(models.keys()),\n    \"top_labels_by_event_ap\": top_labels\n}\nwith open(OUT / \"config.json\", \"w\") as f:\n    json.dump(config, f, indent=2)\n\nsorted([p.name for p in OUT.iterdir()]), sorted([p.name for p in (OUT/\"plots\").iterdir()])[:10]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, glob\nbase = \"/kaggle/working/phase4_artifacts\"\nprint(\"CSVs:\", glob.glob(base + \"/*.csv\"))\nprint(\"Config:\", glob.glob(base + \"/*.json\"))\nprint(\"Models:\", glob.glob(base + \"/models/*.joblib\")[:5], \"…\")\nprint(\"Plots:\", glob.glob(base + \"/plots/*.png\")[:5], \"…\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# summary\n\n- Multi-label models saved under `phase4_artifacts/models/`\n- Pair-level CV metrics and event-level OOF metrics (`metrics_per_label.csv`, `event_metrics_oof.csv`)\n- Operating thresholds (`thresholds.csv`)\n- ROC/PR/Calibration/Drift plots for top labels\n- SHAP explanation plot for the main label\n","metadata":{}}]}