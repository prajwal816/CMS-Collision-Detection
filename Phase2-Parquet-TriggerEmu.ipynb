{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14857240,"sourceType":"datasetVersion","datasetId":9503642}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Phase 2 — Build certified training table (NanoAOD → Parquet)\n\nWe will:\n1) load validated run/lumi JSON (muons-only)\n2) stream ROOT files with uproot.iterate (memory-safe)\n3) build dimuon candidates + features\n4) add trigger labels from `HLT_*`\n5) write Parquet shards for ML\n","metadata":{}},{"cell_type":"code","source":"# Cell 1 — Install deps (no XRootD needed)\n!pip -q install \"uproot>=5\" awkward vector rich tqdm pandas pyarrow fastparquet matplotlib\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T12:06:40.905286Z","iopub.execute_input":"2026-02-16T12:06:40.905976Z","iopub.status.idle":"2026-02-16T12:06:47.855019Z","shell.execute_reply.started":"2026-02-16T12:06:40.905947Z","shell.execute_reply":"2026-02-16T12:06:47.854314Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.8/393.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m919.6/919.6 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m656.7/656.7 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell P2-1 — Config\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport awkward as ak\nimport uproot\nimport vector\n\nvector.register_awkward()\n\nBASE_PATH = Path(\"/kaggle/input/datasets/katakuricharlotte/doublemuon2016g-rootfiles/root_converted\")\nROOT_FILES = sorted(map(str, BASE_PATH.glob(\"*.root\")))\nlen(ROOT_FILES), ROOT_FILES[:3]\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T12:06:04.436402Z","iopub.execute_input":"2026-02-16T12:06:04.436641Z","iopub.status.idle":"2026-02-16T12:06:05.486031Z","shell.execute_reply.started":"2026-02-16T12:06:04.436619Z","shell.execute_reply":"2026-02-16T12:06:05.484921Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2702221320.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mawkward\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muproot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'awkward'"],"ename":"ModuleNotFoundError","evalue":"No module named 'awkward'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# Cell P2-2 — Download validated runs JSON (muons only) and parse\n# The dataset record links validated runs JSONs; for this project we use \"muons only\". [page:14]\nimport json, urllib.request\n\nVALID_JSON_URL = \"https://opendata.cern.ch/record/14221/files/Cert_271036-284044_13TeV_Legacy2016_Collisions16_JSON_MuonPhys.txt\"\njson_path = Path(\"/kaggle/working/validated_runs_muons_2016.txt\")\n\nif not json_path.exists():\n    urllib.request.urlretrieve(VALID_JSON_URL, json_path)\n\nwith open(json_path, \"r\") as f:\n    certified = json.load(f)\n\n# certified is dict: { \"run\": [ [lumiStart, lumiEnd], ... ], ... }\nlen(certified), list(certified.items())[:1]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell P2-3 — Fast certified filter helpers\ndef is_certified(run, lumi, certified_map):\n    # run: int, lumi: int\n    rs = str(int(run))\n    if rs not in certified_map:\n        return False\n    for lo, hi in certified_map[rs]:\n        if lo <= int(lumi) <= hi:\n            return True\n    return False\n\ndef mask_certified(runs, lumis, certified_map):\n    # vectorized-ish for numpy arrays\n    out = np.zeros(len(runs), dtype=bool)\n    for i, (r, l) in enumerate(zip(runs, lumis)):\n        out[i] = is_certified(r, l, certified_map)\n    return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell P2-4 — Choose triggers to emulate (auto-detect, then pick 3)\n# Record 30522 lists possible HLT trigger paths for this dataset. [page:14]\ntest = uproot.open(ROOT_FILES[0])[\"Events\"]\nbranches = test.keys()\n\nhlt = sorted([b for b in branches if b.startswith(\"HLT_\") and \"Mu\" in b])\nhlt[:50], len(hlt)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell P2-5 — Pick 3 practical labels (prefer common dimuon triggers if present)\npreferred = [\n    \"HLT_Mu17_Mu8\",\n    \"HLT_Mu17_Mu8_DZ\",\n    \"HLT_Mu17_TrkIsoVVL_Mu8_TrkIsoVVL_DZ\",\n]\nLABELS = [x for x in preferred if x in branches]\nif len(LABELS) < 1:\n    # fallback: take the first few muon triggers present\n    LABELS = hlt[:3]\n\nLABELS\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell P2-6 — Feature engineering (dimuon-level) + write Parquet shards\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nMUON_MASS = 0.105658\n\nFEATURES = [\n    \"run\", \"luminosityBlock\", \"event\",\n    \"PV_npvs\", \"MET_pt\",\n    \"Muon_pt\", \"Muon_eta\", \"Muon_phi\", \"Muon_charge\",\n    \"Muon_pfRelIso03_all\", \"Muon_tightId\", \"Muon_mediumId\",\n]\n\n# Keep only branches that exist\nBASE_READ = [b for b in FEATURES if b in branches] + LABELS\n\nOUT_DIR = Path(\"/kaggle/working/parquet_dimuon\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\ndef build_dimuon_table(a):\n    # Certified mask at event level\n    runs = ak.to_numpy(a[\"run\"])\n    lumis = ak.to_numpy(a[\"luminosityBlock\"])\n    good = mask_certified(runs, lumis, certified)\n\n    a = a[good]\n\n    # Build vector muons\n    mu = vector.zip({\n        \"pt\": a[\"Muon_pt\"],\n        \"eta\": a[\"Muon_eta\"],\n        \"phi\": a[\"Muon_phi\"],\n        \"mass\": ak.ones_like(a[\"Muon_pt\"]) * MUON_MASS,\n        \"charge\": a[\"Muon_charge\"],\n    })\n\n    # quality: prefer mediumId if present\n    qual = ak.ones_like(a[\"Muon_pt\"], dtype=bool)\n    if \"Muon_mediumId\" in a.fields:\n        qual = qual & (a[\"Muon_mediumId\"] == 1)\n    elif \"Muon_tightId\" in a.fields:\n        qual = qual & (a[\"Muon_tightId\"] == 1)\n\n    mu = mu[qual]\n\n    # Opposite-sign pairs\n    pairs = ak.combinations(mu, 2, fields=[\"m1\", \"m2\"])\n    os_pairs = pairs[(pairs.m1.charge * pairs.m2.charge) < 0]\n    dimu = os_pairs.m1 + os_pairs.m2\n\n    # Build per-pair feature table\n    # Event keys are repeated for each pair\n    out = {\n        \"run\": ak.flatten(ak.broadcast_arrays(a[\"run\"], dimu.mass)[0]),\n        \"lumi\": ak.flatten(ak.broadcast_arrays(a[\"luminosityBlock\"], dimu.mass)[0]),\n        \"event\": ak.flatten(ak.broadcast_arrays(a[\"event\"], dimu.mass)[0]),\n        \"m_mumu\": ak.to_numpy(ak.flatten(dimu.mass)),\n        \"pt_mumu\": ak.to_numpy(ak.flatten(dimu.pt)),\n        \"eta_mumu\": ak.to_numpy(ak.flatten(dimu.eta)),\n        \"dR_mumu\": ak.to_numpy(ak.flatten(os_pairs.m1.deltaR(os_pairs.m2))),\n    }\n\n    # Add event-level context (repeat per pair)\n    if \"PV_npvs\" in a.fields:\n        out[\"PV_npvs\"] = ak.to_numpy(ak.flatten(ak.broadcast_arrays(a[\"PV_npvs\"], dimu.mass)[0]))\n    if \"MET_pt\" in a.fields:\n        out[\"MET_pt\"] = ak.to_numpy(ak.flatten(ak.broadcast_arrays(a[\"MET_pt\"], dimu.mass)[0]))\n\n    # Add trigger labels (repeat per pair)\n    for lab in LABELS:\n        out[lab] = ak.to_numpy(ak.flatten(ak.broadcast_arrays(a[lab], dimu.mass)[0])).astype(np.int8)\n\n    return pd.DataFrame(out)\n\n# Iterate files in chunks (safe for memory)\nMAX_EVENTS_TOTAL = 1_000_000   # scale later\nevents_seen = 0\nshard = 0\n\nfor arrays in uproot.iterate(\n    ROOT_FILES,\n    \"Events\",\n    expressions=BASE_READ,\n    step_size=\"100 MB\",\n    library=\"ak\"\n):\n    df = build_dimuon_table(arrays)\n    if len(df) == 0:\n        continue\n\n    out_path = OUT_DIR / f\"dimuon_shard_{shard:03d}.parquet\"\n    df.to_parquet(out_path, index=False)\n    shard += 1\n\n    events_seen += len(arrays[\"run\"])\n    if events_seen >= MAX_EVENTS_TOTAL:\n        break\n\nshard, events_seen\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}